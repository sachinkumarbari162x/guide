<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Probability & Statistics - Complete Learning Guide</title>
    <link rel="stylesheet" href="probability.css">
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800;900&family=Fira+Code:wght@400;500&display=swap"
        rel="stylesheet">
</head>

<body>

    <!-- Header -->
    <header class="header">
        <div class="logo">üìä Probability & Statistics</div>
        <nav>
            <ul class="nav-links">
                <li><a href="#fundamentals">Fundamentals</a></li>
                <li><a href="#distributions">Distributions</a></li>
                <li><a href="#bayes">Bayes</a></li>
                <li><a href="#inference">Inference</a></li>
                <li><a href="#ml">ML Applications</a></li>
            </ul>
        </nav>
    </header>

    <!-- Hero -->
    <section class="hero" id="hero">
        <div class="hero-badge">üé≤ The Language of Uncertainty</div>
        <h1>Master <span class="gradient">Probability & Statistics</span></h1>
        <p class="hero-subtitle">
            The mathematical framework for reasoning under uncertainty. Essential for machine learning, Bayesian
            inference, and data science.
        </p>
    </section>

    <!-- Section 1: Fundamentals -->
    <section class="section" id="fundamentals">
        <div class="section-header">
            <span class="section-tag">Section 01</span>
            <h2>Probability <span>Fundamentals</span></h2>
            <p class="section-desc">Core concepts that form the foundation of probabilistic reasoning.</p>
        </div>

        <div class="cards-grid">
            <div class="card">
                <div class="card-icon">üé≤</div>
                <h3>Basic Probability</h3>
                <p>Measuring uncertainty with numbers.</p>
                <ul>
                    <li><strong>P(A):</strong> Probability of event A</li>
                    <li><strong>0 ‚â§ P(A) ‚â§ 1</strong></li>
                    <li><strong>P(Œ©) = 1:</strong> Sample space</li>
                    <li><strong>P(‚àÖ) = 0:</strong> Impossible event</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üîÄ</div>
                <h3>Conditional Probability</h3>
                <p>Probability given information.</p>
                <ul>
                    <li><strong>P(A|B) = P(A‚à©B) / P(B)</strong></li>
                    <li>Updated belief after observing B</li>
                    <li>Chain rule: P(A,B) = P(A|B)P(B)</li>
                    <li>Foundation of Bayes' theorem</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">‚ö°</div>
                <h3>Independence</h3>
                <p>Events that don't influence each other.</p>
                <ul>
                    <li><strong>P(A‚à©B) = P(A)P(B)</strong></li>
                    <li>P(A|B) = P(A) if independent</li>
                    <li>Coin flips are independent</li>
                    <li>Crucial for many ML assumptions</li>
                </ul>
            </div>
        </div>

        <div class="math-box">
            <div class="math-label">The Addition Rule</div>
            <div class="math-formula">
                P(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)
            </div>
            <p class="math-note">For mutually exclusive events: P(A ‚à™ B) = P(A) + P(B)</p>
        </div>

        <div class="code-block">
            <div class="code-header">
                <span class="code-dot red"></span>
                <span class="code-dot yellow"></span>
                <span class="code-dot green"></span>
                <span class="code-title">probability_basics.py</span>
            </div>
            <div class="code-body">
                <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> scipy <span class="keyword">import</span> stats

<span class="comment"># Sample space: rolling a die</span>
sample_space = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]

<span class="comment"># Probability of rolling > 4</span>
event_A = [x <span class="keyword">for</span> x <span class="keyword">in</span> sample_space <span class="keyword">if</span> x > <span class="number">4</span>]
P_A = len(event_A) / len(sample_space)  <span class="comment"># 2/6 = 0.333</span>

<span class="comment"># Conditional probability: P(A|B)</span>
<span class="comment"># P(roll > 4 | roll is even)</span>
event_B = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>]  <span class="comment"># even numbers</span>
event_A_and_B = [x <span class="keyword">for</span> x <span class="keyword">in</span> event_A <span class="keyword">if</span> x <span class="keyword">in</span> event_B]  <span class="comment"># [6]</span>
P_A_given_B = len(event_A_and_B) / len(event_B)  <span class="comment"># 1/3</span>

<span class="comment"># Monte Carlo simulation</span>
n_trials = <span class="number">100000</span>
rolls = np.random.randint(<span class="number">1</span>, <span class="number">7</span>, n_trials)
empirical_prob = np.mean(rolls > <span class="number">4</span>)  <span class="comment"># ‚âà 0.333</span></pre>
            </div>
        </div>

        <div class="source-box">
            <h4>üìñ Learning Resources</h4>
            <ul>
                <li><a href="https://www.khanacademy.org/math/statistics-probability" target="_blank">Khan Academy -
                        Statistics & Probability</a></li>
                <li><a href="https://seeing-theory.brown.edu/" target="_blank">Seeing Theory - Visual Intro to
                        Probability</a></li>
                <li><a href="https://ocw.mit.edu/courses/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/"
                        target="_blank">MIT 6.041 - Probabilistic Systems Analysis</a></li>
            </ul>
        </div>
    </section>

    <!-- Section 2: Distributions -->
    <section class="section" id="distributions">
        <div class="section-header">
            <span class="section-tag">Section 02</span>
            <h2>Probability <span>Distributions</span></h2>
            <p class="section-desc">Mathematical functions that describe how probability is distributed over possible
                outcomes.</p>
        </div>

        <div class="cards-grid">
            <div class="card">
                <div class="card-icon">üìà</div>
                <h3>Normal (Gaussian)</h3>
                <p>The "bell curve" - appears everywhere in nature.</p>
                <ul>
                    <li><strong>Parameters:</strong> Œº (mean), œÉ¬≤ (variance)</li>
                    <li>68-95-99.7 rule for standard deviations</li>
                    <li>Central Limit Theorem applies</li>
                    <li>Used in: Neural network initialization</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üéØ</div>
                <h3>Bernoulli & Binomial</h3>
                <p>Binary outcomes and counts of successes.</p>
                <ul>
                    <li><strong>Bernoulli:</strong> Single trial, P(X=1) = p</li>
                    <li><strong>Binomial:</strong> n trials, k successes</li>
                    <li>P(k) = C(n,k) p·µè (1-p)‚Åø‚Åª·µè</li>
                    <li>Used in: Binary classification</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">‚è±Ô∏è</div>
                <h3>Exponential & Poisson</h3>
                <p>Events over time and rare events.</p>
                <ul>
                    <li><strong>Exponential:</strong> Time between events</li>
                    <li><strong>Poisson:</strong> Count of rare events</li>
                    <li>Memoryless property</li>
                    <li>Used in: Queuing, arrivals modeling</li>
                </ul>
            </div>
        </div>

        <!-- Interactive Distribution Visualizer -->
        <div class="canvas-container">
            <div class="canvas-header">
                <span class="canvas-title">üéÆ Interactive: Distribution Visualizer</span>
                <div class="canvas-controls dist-selector">
                    <button class="btn btn-secondary dist-btn active" data-dist="normal">Normal</button>
                    <button class="btn btn-secondary dist-btn" data-dist="exponential">Exponential</button>
                    <button class="btn btn-secondary dist-btn" data-dist="uniform">Uniform</button>
                </div>
            </div>
            <div class="canvas-wrapper">
                <canvas id="distribution-canvas"></canvas>
            </div>
            <div id="normal-params">
                <div class="slider-group">
                    <div class="slider-item">
                        <span class="slider-label">Mean (Œº):</span>
                        <input type="range" id="mean-slider" min="-3" max="3" step="0.1" value="0">
                        <span class="slider-value" id="mean-val">0.0</span>
                    </div>
                    <div class="slider-item">
                        <span class="slider-label">Std Dev (œÉ):</span>
                        <input type="range" id="std-slider" min="0.5" max="3" step="0.1" value="1">
                        <span class="slider-value" id="std-val">1.0</span>
                    </div>
                </div>
            </div>
            <div id="exp-params" style="display: none;">
                <div class="slider-group">
                    <div class="slider-item">
                        <span class="slider-label">Rate (Œª):</span>
                        <input type="range" id="lambda-slider" min="0.5" max="5" step="0.1" value="1">
                        <span class="slider-value" id="lambda-val">1.0</span>
                    </div>
                </div>
            </div>
        </div>

        <div class="math-box">
            <div class="math-label">Normal Distribution PDF</div>
            <div class="math-formula">
                f(x) = (1 / œÉ‚àö(2œÄ)) ¬∑ exp(-(x-Œº)¬≤ / 2œÉ¬≤)
            </div>
        </div>

        <div class="code-block">
            <div class="code-header">
                <span class="code-dot red"></span>
                <span class="code-dot yellow"></span>
                <span class="code-dot green"></span>
                <span class="code-title">distributions.py</span>
            </div>
            <div class="code-body">
                <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> scipy <span class="keyword">import</span> stats
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt

<span class="comment"># Normal distribution</span>
normal = stats.norm(loc=<span class="number">0</span>, scale=<span class="number">1</span>)  <span class="comment"># Œº=0, œÉ=1</span>
<span class="keyword">print</span>(normal.pdf(<span class="number">0</span>))    <span class="comment"># PDF at x=0 ‚âà 0.399</span>
<span class="keyword">print</span>(normal.cdf(<span class="number">1.96</span>)) <span class="comment"># CDF at x=1.96 ‚âà 0.975</span>
samples = normal.rvs(size=<span class="number">1000</span>)  <span class="comment"># Generate samples</span>

<span class="comment"># Binomial distribution</span>
binom = stats.binom(n=<span class="number">10</span>, p=<span class="number">0.3</span>)
<span class="keyword">print</span>(binom.pmf(<span class="number">3</span>))  <span class="comment"># P(X=3) ‚âà 0.267</span>

<span class="comment"># Exponential distribution</span>
exp = stats.expon(scale=<span class="number">1</span>/<span class="number">2</span>)  <span class="comment"># Œª=2</span>
<span class="keyword">print</span>(exp.mean())  <span class="comment"># E[X] = 1/Œª = 0.5</span>

<span class="comment"># Poisson distribution</span>
poisson = stats.poisson(mu=<span class="number">5</span>)
<span class="keyword">print</span>(poisson.pmf(<span class="number">3</span>))  <span class="comment"># P(X=3) ‚âà 0.140</span>

<span class="comment"># Sampling from distributions for ML</span>
<span class="comment"># Xavier/Glorot initialization</span>
fan_in, fan_out = <span class="number">100</span>, <span class="number">50</span>
std = np.sqrt(<span class="number">2</span> / (fan_in + fan_out))
weights = np.random.normal(<span class="number">0</span>, std, (fan_in, fan_out))</pre>
            </div>
        </div>
    </section>

    <!-- Section 3: Bayes' Theorem -->
    <section class="section" id="bayes">
        <div class="section-header">
            <span class="section-tag">Section 03</span>
            <h2>Bayes' <span>Theorem</span></h2>
            <p class="section-desc">The foundation of probabilistic reasoning and updating beliefs with evidence.</p>
        </div>

        <div class="math-box">
            <div class="math-label">Bayes' Theorem</div>
            <div class="math-formula">
                P(A|B) = P(B|A) ¬∑ P(A) / P(B)
            </div>
            <p class="math-note">
                <strong>Posterior</strong> = (Likelihood √ó Prior) / Evidence
            </p>
        </div>

        <div class="cards-grid">
            <div class="card">
                <div class="card-icon">üéØ</div>
                <h3>Prior P(A)</h3>
                <p>Your initial belief before seeing evidence.</p>
                <ul>
                    <li>Based on previous knowledge</li>
                    <li>Can be uniform (no prior belief)</li>
                    <li>Subjective in Bayesian statistics</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üîç</div>
                <h3>Likelihood P(B|A)</h3>
                <p>Probability of evidence given hypothesis.</p>
                <ul>
                    <li>How well data fits the model</li>
                    <li>Maximum Likelihood Estimation (MLE)</li>
                    <li>Core of statistical modeling</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">‚ú®</div>
                <h3>Posterior P(A|B)</h3>
                <p>Updated belief after seeing evidence.</p>
                <ul>
                    <li>Combines prior and likelihood</li>
                    <li>Becomes prior for next update</li>
                    <li>Maximum A Posteriori (MAP)</li>
                </ul>
            </div>
        </div>

        <!-- Bayes Calculator -->
        <div class="canvas-container">
            <div class="canvas-header">
                <span class="canvas-title">üéÆ Interactive: Bayes' Theorem Calculator</span>
            </div>
            <div
                style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 1rem; margin-bottom: 1rem;">
                <div>
                    <label
                        style="display: block; color: var(--text-secondary); font-size: 0.9rem; margin-bottom: 0.5rem;">P(A)
                        - Prior:</label>
                    <input type="number" id="prior-a" value="0.01" step="0.01" min="0" max="1"
                        style="width: 100%; padding: 0.8rem; background: var(--bg-primary); border: 1px solid var(--border-color); border-radius: 8px; color: var(--text-primary); font-family: 'Fira Code';">
                </div>
                <div>
                    <label
                        style="display: block; color: var(--text-secondary); font-size: 0.9rem; margin-bottom: 0.5rem;">P(B|A)
                        - Likelihood:</label>
                    <input type="number" id="likelihood-ba" value="0.99" step="0.01" min="0" max="1"
                        style="width: 100%; padding: 0.8rem; background: var(--bg-primary); border: 1px solid var(--border-color); border-radius: 8px; color: var(--text-primary); font-family: 'Fira Code';">
                </div>
                <div>
                    <label
                        style="display: block; color: var(--text-secondary); font-size: 0.9rem; margin-bottom: 0.5rem;">P(B|¬¨A)
                        - False Positive:</label>
                    <input type="number" id="likelihood-b-nota" value="0.05" step="0.01" min="0" max="1"
                        style="width: 100%; padding: 0.8rem; background: var(--bg-primary); border: 1px solid var(--border-color); border-radius: 8px; color: var(--text-primary); font-family: 'Fira Code';">
                </div>
            </div>
            <button class="btn btn-primary" id="calc-bayes">Calculate Posterior P(A|B)</button>
            <div id="bayes-result"
                style="margin-top: 1.5rem; padding: 1rem; background: var(--bg-primary); border-radius: 8px; min-height: 100px;">
                <div style="color: var(--text-muted);">Enter values and click calculate...</div>
            </div>
            <p style="margin-top: 1rem; color: var(--text-muted); font-size: 0.9rem;">
                Example: Disease testing with 1% prevalence, 99% sensitivity, 5% false positive rate.
            </p>
        </div>

        <div class="code-block">
            <div class="code-header">
                <span class="code-dot red"></span>
                <span class="code-dot yellow"></span>
                <span class="code-dot green"></span>
                <span class="code-title">bayes_theorem.py</span>
            </div>
            <div class="code-body">
                <pre><span class="keyword">def</span> <span class="function">bayes_theorem</span>(prior, likelihood, false_positive):
    <span class="string">"""Calculate posterior probability using Bayes' theorem"""</span>
    <span class="comment"># P(B) = P(B|A)P(A) + P(B|¬¨A)P(¬¨A)</span>
    p_b = likelihood * prior + false_positive * (<span class="number">1</span> - prior)
    
    <span class="comment"># P(A|B) = P(B|A)P(A) / P(B)</span>
    posterior = (likelihood * prior) / p_b
    <span class="keyword">return</span> posterior

<span class="comment"># Medical testing example</span>
<span class="comment"># Disease prevalence: 1%, Test sensitivity: 99%, False positive: 5%</span>
posterior = bayes_theorem(<span class="number">0.01</span>, <span class="number">0.99</span>, <span class="number">0.05</span>)
<span class="keyword">print</span>(<span class="string">f"P(Disease|Positive Test) = {posterior:.2%}"</span>)  <span class="comment"># ‚âà 16.7%!</span>

<span class="comment"># Bayesian updating (sequential)</span>
<span class="keyword">def</span> <span class="function">bayesian_update</span>(priors, likelihoods):
    <span class="string">"""Update beliefs with multiple observations"""</span>
    posterior = priors.copy()
    <span class="keyword">for</span> likelihood <span class="keyword">in</span> likelihoods:
        evidence = sum(p * l <span class="keyword">for</span> p, l <span class="keyword">in</span> zip(posterior, likelihood))
        posterior = [(p * l) / evidence <span class="keyword">for</span> p, l <span class="keyword">in</span> zip(posterior, likelihood)]
    <span class="keyword">return</span> posterior</pre>
            </div>
        </div>
    </section>

    <!-- Section 4: Statistical Inference -->
    <section class="section" id="inference">
        <div class="section-header">
            <span class="section-tag">Section 04</span>
            <h2>Statistical <span>Inference</span></h2>
            <p class="section-desc">Drawing conclusions about populations from samples.</p>
        </div>

        <div class="cards-grid">
            <div class="card">
                <div class="card-icon">üìä</div>
                <h3>Central Limit Theorem</h3>
                <p>The cornerstone of statistics.</p>
                <ul>
                    <li>Sample means ‚Üí Normal distribution</li>
                    <li>Works for any population distribution</li>
                    <li>Standard error = œÉ / ‚àön</li>
                    <li>Enables confidence intervals</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üéØ</div>
                <h3>Confidence Intervals</h3>
                <p>Range of plausible population values.</p>
                <ul>
                    <li>95% CI: xÃÑ ¬± 1.96 √ó SE</li>
                    <li>Interpretation: 95% of CIs contain Œº</li>
                    <li>Width decreases with ‚àön</li>
                    <li>Critical for A/B testing</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">‚öñÔ∏è</div>
                <h3>Hypothesis Testing</h3>
                <p>Making decisions with data.</p>
                <ul>
                    <li>H‚ÇÄ: Null hypothesis (no effect)</li>
                    <li>H‚ÇÅ: Alternative hypothesis</li>
                    <li>p-value: P(data | H‚ÇÄ)</li>
                    <li>Œ± = 0.05 significance level</li>
                </ul>
            </div>
        </div>

        <!-- CLT Simulator -->
        <div class="canvas-container">
            <div class="canvas-header">
                <span class="canvas-title">üéÆ Interactive: Central Limit Theorem</span>
                <div class="canvas-controls">
                    <button class="btn btn-primary" id="gen-clt">Generate Samples</button>
                </div>
            </div>
            <div class="canvas-wrapper">
                <canvas id="clt-canvas"></canvas>
            </div>
            <div class="slider-group">
                <div class="slider-item">
                    <span class="slider-label">Sample Size (n):</span>
                    <input type="range" id="sample-size" min="5" max="100" step="5" value="30">
                    <span class="slider-value" id="sample-size-val">30</span>
                </div>
            </div>
            <p style="text-align: center; color: var(--text-muted); margin-top: 1rem;">
                Samples from Uniform[0,1] distribution. Pink curve shows the normal approximation (CLT in action!).
            </p>
        </div>

        <!-- Hypothesis Testing Demo -->
        <div class="canvas-container">
            <div class="canvas-header">
                <span class="canvas-title">üéÆ Interactive: Hypothesis Testing</span>
                <div class="canvas-controls">
                    <button class="btn btn-primary" id="run-hypothesis">Run Test</button>
                </div>
            </div>
            <div id="hypothesis-output"
                style="background: var(--bg-primary); border-radius: 8px; padding: 1.5rem; min-height: 200px;">
                <div style="color: var(--text-muted);">Click "Run Test" to see a step-by-step t-test...</div>
            </div>
        </div>

        <div class="math-box">
            <div class="math-label">The Standard Error</div>
            <div class="math-formula">
                SE = œÉ / ‚àön
            </div>
            <p class="math-note">As sample size increases, the standard error decreases, giving more precise estimates.
            </p>
        </div>
    </section>

    <!-- Section 5: ML Applications -->
    <section class="section" id="ml">
        <div class="section-header">
            <span class="section-tag">Section 05</span>
            <h2>Applications in <span>Machine Learning</span></h2>
            <p class="section-desc">Probability and statistics power every ML algorithm.</p>
        </div>

        <div class="cards-grid">
            <div class="card">
                <div class="card-icon">üéØ</div>
                <h3>Maximum Likelihood</h3>
                <p>Find parameters that maximize data probability.</p>
                <ul>
                    <li>Œ∏* = argmax P(data | Œ∏)</li>
                    <li>Log-likelihood for numerical stability</li>
                    <li>Basis of logistic regression</li>
                    <li>Cross-entropy loss connection</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üß†</div>
                <h3>Probabilistic Models</h3>
                <p>Modeling uncertainty in predictions.</p>
                <ul>
                    <li>Naive Bayes classifier</li>
                    <li>Gaussian Mixture Models</li>
                    <li>Bayesian Neural Networks</li>
                    <li>Variational Autoencoders</li>
                </ul>
            </div>
            <div class="card">
                <div class="card-icon">üìâ</div>
                <h3>Regularization</h3>
                <p>Priors as regularization.</p>
                <ul>
                    <li>L2 = Gaussian prior on weights</li>
                    <li>L1 = Laplace prior (sparsity)</li>
                    <li>Dropout = Bayesian approximation</li>
                    <li>MAP estimation</li>
                </ul>
            </div>
        </div>

        <div class="code-block">
            <div class="code-header">
                <span class="code-dot red"></span>
                <span class="code-dot yellow"></span>
                <span class="code-dot green"></span>
                <span class="code-title">ml_probability.py</span>
            </div>
            <div class="code-body">
                <pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB

<span class="comment"># Maximum Likelihood Estimation for Gaussian</span>
<span class="keyword">def</span> <span class="function">mle_gaussian</span>(data):
    <span class="string">"""MLE estimates for Normal distribution"""</span>
    mu_mle = np.mean(data)
    sigma_mle = np.std(data, ddof=<span class="number">0</span>)  <span class="comment"># MLE uses n, not n-1</span>
    <span class="keyword">return</span> mu_mle, sigma_mle

<span class="comment"># Log-likelihood (numerical stability)</span>
<span class="keyword">def</span> <span class="function">log_likelihood</span>(data, mu, sigma):
    n = len(data)
    ll = -n/<span class="number">2</span> * np.log(<span class="number">2</span> * np.pi * sigma**<span class="number">2</span>)
    ll -= np.sum((data - mu)**<span class="number">2</span>) / (<span class="number">2</span> * sigma**<span class="number">2</span>)
    <span class="keyword">return</span> ll

<span class="comment"># Naive Bayes Classifier</span>
X_train = np.random.randn(<span class="number">100</span>, <span class="number">5</span>)
y_train = (X_train[:, <span class="number">0</span>] > <span class="number">0</span>).astype(<span class="keyword">int</span>)

nb = GaussianNB()
nb.fit(X_train, y_train)

<span class="comment"># Predict probabilities (not just class)</span>
X_test = np.random.randn(<span class="number">5</span>, <span class="number">5</span>)
probs = nb.predict_proba(X_test)  <span class="comment"># P(class | features)</span>

<span class="comment"># Cross-entropy loss = negative log-likelihood</span>
<span class="keyword">def</span> <span class="function">cross_entropy</span>(y_true, y_pred):
    epsilon = <span class="number">1e-15</span>  <span class="comment"># Prevent log(0)</span>
    y_pred = np.clip(y_pred, epsilon, <span class="number">1</span> - epsilon)
    <span class="keyword">return</span> -np.mean(y_true * np.log(y_pred) + 
                    (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred))</pre>
            </div>
        </div>

        <div class="source-box">
            <h4>üìñ Deep Dive Resources</h4>
            <ul>
                <li><a href="https://www.deeplearningbook.org/contents/prob.html" target="_blank">Deep Learning Book -
                        Probability</a></li>
                <li><a href="https://www.stat.cmu.edu/~larry/all-of-statistics/" target="_blank">All of Statistics
                        (Wasserman)</a></li>
                <li><a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/" target="_blank">Cornell CS4780
                        - Machine Learning</a></li>
            </ul>
        </div>
    </section>

    <!-- Learning Roadmap -->
    <section class="section" id="roadmap">
        <div class="section-header">
            <span class="section-tag">üéØ Learning Path</span>
            <h2>Your <span>Statistics Journey</span></h2>
        </div>

        <div class="learning-path">
            <div class="path-line"></div>

            <div class="path-item">
                <div class="path-marker">1</div>
                <div class="path-content">
                    <h4>Week 1-2: Probability Basics</h4>
                    <p>Sample spaces, events, conditional probability, independence.</p>
                </div>
            </div>

            <div class="path-item">
                <div class="path-marker">2</div>
                <div class="path-content">
                    <h4>Week 3-4: Distributions</h4>
                    <p>Discrete and continuous distributions, PDFs, CDFs, expected value.</p>
                </div>
            </div>

            <div class="path-item">
                <div class="path-marker">3</div>
                <div class="path-content">
                    <h4>Week 5-6: Inference</h4>
                    <p>Central Limit Theorem, confidence intervals, hypothesis testing.</p>
                </div>
            </div>

            <div class="path-item">
                <div class="path-marker">4</div>
                <div class="path-content">
                    <h4>Week 7-8: Bayesian Methods</h4>
                    <p>Bayes' theorem, prior/posterior, Bayesian updating.</p>
                </div>
            </div>

            <div class="path-item">
                <div class="path-marker">5</div>
                <div class="path-content">
                    <h4>Week 9+: ML Applications</h4>
                    <p>MLE, probabilistic models, information theory.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer class="footer">
        <p>Built with üíú for learners | Master probability, master machine learning</p>
        <p style="margin-top: 1rem; font-size: 0.8rem;">
            "Probability theory is nothing but common sense reduced to calculation." - Laplace
        </p>
    </footer>

    <script src="probability.js"></script>

</body>

</html>